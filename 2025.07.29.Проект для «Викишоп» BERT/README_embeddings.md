# Проект для «Викишоп» BERT
Учебный проект

## Цель проекта:
обучить модель классифицировать комментарии пользователей на позитивные и негативные.

## Описание данных:
Данные представляют собой датафрэйм из столбца с комментариями длиной до 4950 токенов и столбца с меткой токсичен/нетоксичен. Токсичных комментариев 10%. Наиболее распространенные слова: "article", "page", "wikipedia"...

## Использовала библиотеки:
- import string
- import torch
- import transformers
- import numpy as np
- import pandas as pd
- import os
- import time
- import torch.nn as nn
- import matplotlib.pyplot as plt
- import seaborn as sns
- import nltk

## Общий вывод:
*Таким образом, данные загружены и изучены, проведен частотный анализ слов. Датафрэйм с комментариями преобразованы в эмбеддинги с помощью модели toxic-bert. Эмбеддинги собраны в матрицу размером 159270 х 512. Часть эмбеддингов потеряна при токенизации батчами, два процента эмбеддингов обрезаны до 512 токенов.*

*На этой матрице обучены четыре модели: DummyClassifier, LogisticRegression, CatBoost, DistilBERT (нейронная сеть). "Тупая" модель c предсказаниями наименьшего класса показала метрику F1=0,184.*

*Лучшее качество показала модель CatBoost {'min_data_in_leaf': 1, 'learning_rate': 0.1, 'l2_leaf_reg': 5, 'grow_policy': 'Lossguide', 'depth': 7, 'border_count': 254} с метрикой F1 = 0,95.*

*Модель полностью и точно определяет практически все токсичные комментарии. Именно создание эмбеддингов на toxic-bert привело к хорошему качеству модели.*

*По итогу проекта выведена часть таблицы с истинными и предсказанными метками токсичности.*


